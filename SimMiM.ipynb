{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da70b3a",
   "metadata": {},
   "source": [
    "# Check GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb805c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de50dfb",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import shutup; shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for heatmaps\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ecb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13951a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_data import *\n",
    "from data import *\n",
    "from data.transform.utils import *\n",
    "\n",
    "from simmim.vision_transformer import ViT\n",
    "from simmim.simmim import SimMIM\n",
    "from pretrain import *\n",
    "\n",
    "\n",
    "from simmim.optimizer import build_pretrain_optimizer, build_finetune_optimizer\n",
    "from simmim.lr_scheduler import build_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad817aa3",
   "metadata": {},
   "source": [
    "# Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed = 42\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ed943",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type =  'spectrogram'            # 'spectrogram', 'time-series'\n",
    "num_workers = 4\n",
    "y_sampling = None                # option: None,'oversampling','undersampling'\n",
    "activities = []\n",
    "sampling = 'weight'\n",
    "batch_size = 64\n",
    "namings = ['exp_15_pwr_spectrograms', 'exp_10_amp_spec_only_STFT', 'exp_11_phdiff_spec_only_STFT']#, 'MarkovTransitionField', 'exp_7_amp_spec_only']#, 'exp_9_phdiff_spec_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f0329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multimodal_data = import_multiple_modalities(data_type = data_type, namings = namings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'multimodal_spectrogram'\n",
    "views = 'associated'\n",
    "axis = 3\n",
    "\n",
    "if data_type == 'multimodal_spectrogram':\n",
    "    X_train, X_test, y_train, y_test = split_multimodal_data(multimodal_data, views = views, axis = axis)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, lb = filtering_activities_and_label_encoding(X_train, X_test, y_train, y_test, \n",
    "                                                                                       activities)\n",
    "del multimodal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693edb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, valid_loader, _ = combine1(X_train, X_test, y_train, y_test, \n",
    "                                                    sampling, lb, batch_size, num_workers,\n",
    "                                                    y_sampling='None')\n",
    "pretrain_set = DataLoader(\n",
    "    X_train,\n",
    "    batch_size       = 64,\n",
    "    shuffle          = True,\n",
    "    drop_last        = True,\n",
    "    num_workers      = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a41b4a",
   "metadata": {},
   "source": [
    "# Visualise modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a6c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (data_type == 'multimodal_spectrogram'):\n",
    "    for i in range(int(pretrain_set.dataset.shape[-1]/224)):\n",
    "        plt.figure()\n",
    "        plt.imshow(pretrain_set.dataset[0][0][0:224, 224*i:(224*(i+1))], cmap = 'jet', origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ff5cf",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60467a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.hybridvit import *\n",
    "from simmim.simmim_cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, pretrain_set.dataset[0].shape[2])\n",
    "patch_size = 224 # [32,32]  [16,16]\n",
    "in_channels = 1\n",
    "num_classes = 6\n",
    "dim = 512\n",
    "depth = 3\n",
    "n_heads = 4\n",
    "mlp_dim = 512\n",
    "dropout = 0.1\n",
    "emb_dropout = 0.1\n",
    "n_filter_list = [1, 16, 32, 64]\n",
    "seq_pool = False\n",
    "positional_embedding = True\n",
    "\n",
    "# Training settings\n",
    "epochs = 500\n",
    "lr = 5e-4\n",
    "multi_gpus = False\n",
    "weight_decay = 0.05\n",
    "network = 'hyb'\n",
    "exp_name = 'SiMMiM_STFT_PWR_0.6_masking'\n",
    "\n",
    "def get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "              n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "              seq_pool, positional_embedding, network):\n",
    "    \n",
    "    if network == 'hyb':\n",
    "        model  = HybridViT(\n",
    "                image_size = img_size, \n",
    "                patch_size = patch_size, \n",
    "                num_classes = num_classes, \n",
    "                dim = dim, \n",
    "                depth = depth, \n",
    "                heads = n_heads,\n",
    "                mlp_dim = mlp_dim,\n",
    "                channels = in_channels,\n",
    "                dropout = dropout,\n",
    "                n_filter_list = n_filter_list,\n",
    "                emb_dropout = emb_dropout,\n",
    "                seq_pool = seq_pool, \n",
    "                positional_embedding = positional_embedding\n",
    "            )\n",
    "    if network == 'vit':\n",
    "        model = ViT(\n",
    "            image_size = img_size, \n",
    "            patch_size = patch_size, \n",
    "            num_classes = num_classes, \n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = n_heads,\n",
    "            mlp_dim = mlp_dim,\n",
    "            channels = in_channels,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "    mim = SimMIM(\n",
    "        encoder = model,\n",
    "        masking_ratio = 0.6  # they found 50% to yield the best results\n",
    "    )\n",
    "    \n",
    "    return mim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                   n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                   seq_pool, positional_embedding, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f1af4",
   "metadata": {},
   "source": [
    "# Pre-training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d776c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                   n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                   seq_pool, positional_embedding, network)\n",
    "\n",
    "#Run or multiple GPUs\n",
    "if multi_gpus == True:\n",
    "    simmim = nn.DataParallel(model, list(range(torch.cuda.device_count())), output_device = 0)\n",
    "    simmim = simmim.to(f'cuda:{model.device_ids[0]}')\n",
    "else:\n",
    "    simmim = simmim.to(device)\n",
    "\n",
    "optimizer = build_pretrain_optimizer(1e-8, (0.9, 0.999), lr, weight_decay, simmim)\n",
    "\n",
    "lr_scheduler = build_scheduler(scheduler = 'multistep', num_epochs = epochs, warmup_epochs = 10, optimizer = optimizer, \n",
    "                               num_batches = len(pretrain_set), decay_rate = 0.1, decay_epochs = 30)\n",
    "\n",
    "simmim, record = pretrain(simmim, optimizer, lr_scheduler, epochs, pretrain_set, valid_loader, device, exp_name, lb, embedding = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(f'{source_dir}/results/saved_models/pretrain/' + exp_name)\n",
    "shutil.rmtree(f'{source_dir}/logs/pretrain/' + exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58569605",
   "metadata": {},
   "source": [
    "# Unsupervised deep clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d9ef3",
   "metadata": {},
   "source": [
    "# Evaluation of the pre-trained model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556e1ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, simmim.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9d92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(1,epochs+1),record['train_loss'])\n",
    "plt.plot(np.arange(1,epochs+1),record['val_loss'])\n",
    "plt.legend(['Training Loss' , 'Validation Loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e27b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "model = simmim.encoder\n",
    "model_dir = f'{source_dir}/results/saved_models/pretrain/' + exp_name + '/'\n",
    "model.load_state_dict(torch.load(model_dir + os.listdir(model_dir)[0]), strict = False)\n",
    "\n",
    "if multi_gpus == True:\n",
    "    cmtx,cls = evaluation(model.module.cpu(), valid_loader, label_encoder = lb)\n",
    "else:\n",
    "    cmtx,cls = evaluation(model.cpu(), valid_loader, label_encoder = lb)\n",
    "    \n",
    "df = ( cmtx.div( cmtx.sum(1).tolist(),axis=0)).round(2)\n",
    "df.columns = df.columns.str.replace(r'predict :', '')\n",
    "df.index    = df.index.str.replace(r'actual:', '')\n",
    "\n",
    "CMAP = 'Blues'\n",
    "FMT = 'g'\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df,cmap=CMAP,annot=True, fmt=FMT)\n",
    "plt.title('')\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c0a61",
   "metadata": {},
   "source": [
    "# Deep Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eb756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                   n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                   seq_pool, positional_embedding, network)\n",
    "\n",
    "model = simmim.encoder\n",
    "\n",
    "model_dir = f'{source_dir}/results/saved_models/pretrain/' + exp_name + '/'\n",
    "\n",
    "model.load_state_dict(torch.load(model_dir + os.listdir(model_dir)[0]), strict = False)\n",
    "\n",
    "model.mlp_head = nn.Identity()\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "embed = model(valid_loader.dataset[:][0].cpu()).cpu()\n",
    "\n",
    "hle = umap.UMAP(\n",
    "    random_state=0,\n",
    "    metric= 'euclidean',\n",
    "    n_components = 25,\n",
    "    n_neighbors = 10,\n",
    "    min_dist= 0.0).fit_transform(embed.detach().numpy())\n",
    "\n",
    "sc = SpectralClustering(\n",
    "            n_clusters= 6,\n",
    "            random_state=42,\n",
    "            affinity='nearest_neighbors')\n",
    "y_pred = sc.fit_predict(hle)\n",
    "\n",
    "y_true = valid_loader.dataset[:][1].detach().numpy()\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy.\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    ind = np.transpose(np.asarray(ind))\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size \n",
    "\n",
    "acc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22874e",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                   n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                   seq_pool, positional_embedding, network)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, simmim.encoder.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "\n",
    "model_parameters = pd.DataFrame({\n",
    "    'img_size'      : [img_size],\n",
    "    'patch_size'    : [patch_size], \n",
    "    'in_channels'   : in_channels,\n",
    "    'num_classes'   : num_classes,\n",
    "    'dim'           : dim,\n",
    "    'depth'         : depth,\n",
    "    'n_heads'       : n_heads,\n",
    "    'mlp_dim'       : mlp_dim\n",
    "})\n",
    "\n",
    "parameters = {\n",
    "    'num_parameters' : [params],\n",
    "    'learning rate' : [lr],\n",
    "    'optimizer' : ['AdamW'],\n",
    "    'Weight decay': [str(0.01)],\n",
    "    'Scheduler' : ['StepLR'],\n",
    "    }\n",
    "\n",
    "num_parameters = pd.DataFrame(parameters)\n",
    "\n",
    "\n",
    "my_yticks = ['PWR1', 'PWR2', 'PWR3', 'AMP_STFT_NUC1', 'AMP_STFT_NUC2', 'PHDIFF_STFT_NUC1', \\\n",
    "             'PHDIFF_STFT_NUC2', 'MTF_DIFF_N1', 'MTF_DIFF_N2', 'MTF_DWT_N1', 'MTF_DWT_N2', \\\n",
    "             'AMP_SCAL_N1', 'AMP_SCAL_N2', 'AMP_DIFF_N1', 'AMP_DIFF_N2']\n",
    "\n",
    "modalities = {}\n",
    "for i, modality in enumerate(my_yticks[0:15 ]):\n",
    "    modalities['modality ' + str(i)] = [modality]\n",
    "data = pd.DataFrame(modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6522153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248d427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finetune import finetune, evaluation, cmtx_table, save_model, record_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                   n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                   seq_pool, positional_embedding, network).to(device)\n",
    "model = simmim.encoder\n",
    "del simmim\n",
    "model_dir = f'{source_dir}/results/saved_models/pretrain/' + exp_name + '/'\n",
    "\n",
    "model.load_state_dict(torch.load(model_dir + os.listdir(model_dir)[0]), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee099c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################################### Lab-Finetuning-phase #####################################################\n",
    "\n",
    "epochs = 50 # 200\n",
    "layer_decay = 0.1\n",
    "base_lr = 1e-3\n",
    "eps = 1e-8\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 0.05\n",
    "depth = depth\n",
    "warmup_epochs = 10 # 10\n",
    "decay_rate = None\n",
    "decay_epochs = 20 # 10\n",
    "dropout = 0.1\n",
    "emb_dropout = 0.1\n",
    "\n",
    "# sampling condition\n",
    "samplings = [1,\n",
    "             int(0.025 * len(X_train) // 6),\n",
    "             int(0.05 * len(X_train) // 6),\n",
    "             int(0.10 * len(X_train) // 6),\n",
    "             int(0.15 * len(X_train) // 6),\n",
    "             int(0.20 * len(X_train) // 6),\n",
    "             #'weight'  ## weight = full training labels used  \n",
    "             ]\n",
    "\n",
    "size_train_exp_name = ['1_img_per_class', '0.025', '0.05', '0.10', '0.15', '0.20'] # , 'all'\n",
    "\n",
    "if not os.path.isdir(f'{source_dir}/results/saved_models/finetune/' + exp_name):\n",
    "    os.mkdir(f'{source_dir}/results/saved_models/finetune/' + exp_name)\n",
    "    os.mkdir(f'{source_dir}/results/records/finetune/' + exp_name)\n",
    "    os.mkdir(f'{source_dir}/logs/finetune/' + exp_name)\n",
    "\n",
    "for i, sampling in enumerate(samplings):\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    simmim = get_model(img_size, patch_size, in_channels, num_classes, dim, depth, \n",
    "                       n_heads, mlp_dim, dropout, emb_dropout, n_filter_list, \n",
    "                       seq_pool, positional_embedding, network).to(device)\n",
    "    \n",
    "    model = simmim.encoder\n",
    "    del simmim\n",
    "    model_dir = f'{source_dir}/results/saved_models/pretrain/' + exp_name + '/'\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_dir + os.listdir(model_dir)[0]), strict = False)\n",
    "\n",
    "    print('\\n\\nsampling: ', sampling, '\\n')\n",
    "    \n",
    "    exp_name_ft = exp_name + '/' + size_train_exp_name[i]\n",
    "    \n",
    "    if sampling != 'weight':\n",
    "        if sampling < 20:\n",
    "            batch_size = 16\n",
    "        if sampling > 20:\n",
    "            batch_size = 32\n",
    "    else:\n",
    "        batch_size = 64\n",
    "        \n",
    "    # create dataloader class \n",
    "    lab_finetune_loader, lab_validatn_loader, class_weight = combine1(X_train, X_test,\n",
    "                                                                      y_train, y_test, \n",
    "                                                                      sampling, lb, batch_size, num_workers, \n",
    "                                                                      y_sampling = y_sampling)\n",
    "    \n",
    "    print(\"class: \", lb.classes_)\n",
    "    print(\"class_size: \", 1 - class_weight)\n",
    "\n",
    "        \n",
    "    if len(lab_finetune_loader.dataset) // batch_size >=1:\n",
    "        n_batches = len(lab_finetune_loader.dataset) // batch_size\n",
    "    else:\n",
    "        n_batches = 1\n",
    "        \n",
    "    # criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device)      \n",
    "\n",
    "    # optimizer\n",
    "    optimizer = build_finetune_optimizer(layer_decay = layer_decay, base_lr = base_lr, epsilon = eps, \n",
    "                             betas = betas, depth = depth, weight_decay = weight_decay, model = model)\n",
    "    # lr_scheduler\n",
    "    lr_scheduler = build_scheduler(scheduler = 'cosinelr', num_epochs = epochs, warmup_epochs = warmup_epochs, \n",
    "                                   optimizer = optimizer, num_batches = n_batches, decay_rate = decay_rate, \n",
    "                                   decay_epochs = decay_epochs)\n",
    "\n",
    "    model, record = finetune(model, criterion, lr_scheduler, optimizer, epochs, lab_finetune_loader, lab_validatn_loader, device, exp_name_ft, lb, embedding = 'no')\n",
    "    \n",
    "    ################################### SAVE RESULTS ################################################\n",
    "    \n",
    "    model_dir = f'{source_dir}/results/saved_models/finetune/' + exp_name_ft + '/'\n",
    "    model.load_state_dict(torch.load(model_dir + os.listdir(model_dir)[0]), strict = False)\n",
    "    \n",
    "    # finetuning \n",
    "    \n",
    "    cmtx, cls = evaluation(model, lab_validatn_loader, label_encoder = lb)        \n",
    "    \n",
    "    metrics = pd.DataFrame({'accuracy': [cls['accuracy'][0]], 'precision': [cls['macro avg']['precision']], \n",
    "                        'recall': [cls['macro avg']['recall']], 'f1-score': [cls['macro avg']['f1-score']]})\n",
    "    \n",
    "    record_log(exp_name_ft, metrics, model_parameters, data, num_parameters)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    del model, criterion, optimizer, record, cmtx, cls, lr_scheduler\n",
    "    del lab_finetune_loader, lab_validatn_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, criterion, optimizer, lr_scheduler\n",
    "del lab_finetune_loader, lab_validatn_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa926aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(f'{source_dir}/results/saved_models/finetune/' + exp_name)\n",
    "shutil.rmtree(f'{source_dir}/logs/finetune/' + exp_name)\n",
    "shutil.rmtree(f'{source_dir}/results/records/finetune/' + exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('Opera')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c7fc84fd92c3328e2ceff7d468a9f0a8d0916594f1fa33dd17910ff09f08f03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
